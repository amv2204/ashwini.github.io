<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>R Programming Code</title>

<script src="site_libs/header-attrs-2.20/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Statistical Software Coding
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="tidy_data.html">R programming</a>
    </li>
    <li class="dropdown-header">SAS programming</li>
    <li class="dropdown-header">GIS</li>
    <li>
      <a href="dashboard.html">Interactive Flex-Dashboard example</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Deliverables from Experiences
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="poster.pdf">NYP Cancer Center Poster</a>
    </li>
    <li>
      <a href="Abstract.pdf">NJ DOH Abstract</a>
    </li>
    <li>
      <a href="NHANES.pdf">NHANES Analysis using SAS</a>
    </li>
    <li>
      <a href="CBPR.pdf">CBPR Initiative</a>
    </li>
    <li>
      <a href="sweet.pdf">Artificial Sweetners and water analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="experience.html">Resume</a>
</li>
<li>
  <a href="mailto:ashwini.varghese@gmail.com">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/ashwinimvarghese">
    <span class="fa fa-brands fa-linkedin"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/amv2204/ashwini.github.io.git">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">R Programming Code</h1>

</div>


<div id="preparation" class="section level2">
<h2>Preparation:</h2>
<p>We’ll start by having a code chunk in the beginning that loads all
the packages we will need and set up formatting for visualizations:</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.4.1      ✔ purrr   1.0.1 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.3.0      ✔ stringr 1.5.0 
## ✔ readr   2.1.3      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(readxl)
library(p8105.datasets)
library(hexbin)
library(ggridges)
library(patchwork)
library(mgcv)</code></pre>
<pre><code>## Loading required package: nlme
## 
## Attaching package: &#39;nlme&#39;
## 
## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse
## 
## This is mgcv 1.8-41. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;.</code></pre>
<pre class="r"><code>library(modelr)
library(viridis)</code></pre>
<pre><code>## Loading required package: viridisLite</code></pre>
<pre class="r"><code>knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = &quot;90%&quot;
)

theme_set(theme_minimal() + theme(legend.position = &quot;bottom&quot;))

options(
  ggplot2.continuous.colour = &quot;viridis&quot;,
  ggplot2.continuous.fill = &quot;viridis&quot;
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d</code></pre>
</div>
<div id="data-cleaning-and-tidying" class="section level2">
<h2>Data cleaning and Tidying:</h2>
<div id="example-1-nyc-transit-dataset-cleaning" class="section level3">
<h3>Example 1: NYC Transit dataset cleaning:</h3>
<p>This dataset contains information related to each entrance and exit
for each subway station in NYC.</p>
<p>We will start by reading in the datafile using the <code>readr</code>
function from the <code>tidyverse</code> package and cleaning the data
by using the <code>clean_names</code> function from the
<code>janitor</code> package. We will also retain certain variables and
convert the entry variable from a character variable to a logical
variable.</p>
<pre class="r"><code>subway = read_csv(&quot;./data/Subway.csv&quot;) %&gt;% 
    janitor::clean_names() %&gt;% 
    select(line, station_name, station_latitude, station_longitude, 
      starts_with(&quot;route&quot;), entry, exit_only, vending, entrance_type, ada) %&gt;% 
    mutate(entry = ifelse(entry == &quot;YES&quot;, TRUE, FALSE)) %&gt;% 
    mutate(route8 = as.character(route8)) %&gt;%  
    mutate(route9 = as.character(route9)) %&gt;% 
    mutate(route10 = as.character(route10)) %&gt;% 
    mutate(route11 = as.character(route11)) </code></pre>
<pre><code>## Rows: 1868 Columns: 32
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
## lgl  (2): ADA, Free Crossover
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code>subway</code></pre>
<pre><code>## # A tibble: 1,868 × 20
##    line     station_…¹ stati…² stati…³ route1 route2 route3 route4 route5 route6
##    &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; 
##  1 4 Avenue 25th St       40.7   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  2 4 Avenue 25th St       40.7   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  3 4 Avenue 36th St       40.7   -74.0 N      R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  4 4 Avenue 36th St       40.7   -74.0 N      R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  5 4 Avenue 36th St       40.7   -74.0 N      R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  6 4 Avenue 45th St       40.6   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  7 4 Avenue 45th St       40.6   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  8 4 Avenue 45th St       40.6   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
##  9 4 Avenue 45th St       40.6   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
## 10 4 Avenue 53rd St       40.6   -74.0 R      &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;  
## # … with 1,858 more rows, 10 more variables: route7 &lt;chr&gt;, route8 &lt;chr&gt;,
## #   route9 &lt;chr&gt;, route10 &lt;chr&gt;, route11 &lt;chr&gt;, entry &lt;lgl&gt;, exit_only &lt;chr&gt;,
## #   vending &lt;chr&gt;, entrance_type &lt;chr&gt;, ada &lt;lgl&gt;, and abbreviated variable
## #   names ¹​station_name, ²​station_latitude, ³​station_longitude</code></pre>
<p>This dataset contains 20 columns and 1868 rows It has the 20
variables that we selected it to keep. We imported the file, used the
<code>clean_names</code> function to do a quick clean. Then we selected
what variables we wanted to keep. Some of the route variables were in
<code>dbl</code> format instead of <code>chr</code> like most of the
route variables so we changed that. And lastly we turned the entry
variable from character into a logical variable.</p>
<p>This data is not tidy because the route variables should be converted
from a wide to long format.</p>
<p>We can use the following code to find the number of distinct
stations:</p>
<pre class="r"><code>subway %&gt;% 
  select(station_name, line) %&gt;% 
  distinct</code></pre>
<pre><code>## # A tibble: 465 × 2
##    station_name             line    
##    &lt;chr&gt;                    &lt;chr&gt;   
##  1 25th St                  4 Avenue
##  2 36th St                  4 Avenue
##  3 45th St                  4 Avenue
##  4 53rd St                  4 Avenue
##  5 59th St                  4 Avenue
##  6 77th St                  4 Avenue
##  7 86th St                  4 Avenue
##  8 95th St                  4 Avenue
##  9 9th St                   4 Avenue
## 10 Atlantic Av-Barclays Ctr 4 Avenue
## # … with 455 more rows</code></pre>
<p>There are 465 distinct stations.</p>
<p>We can use the following code to find the number of ADA compliant
stations:</p>
<pre class="r"><code>subway %&gt;% 
  filter(ada == TRUE) %&gt;% 
  select(station_name, line) %&gt;% 
  distinct</code></pre>
<pre><code>## # A tibble: 84 × 2
##    station_name                   line           
##    &lt;chr&gt;                          &lt;chr&gt;          
##  1 Atlantic Av-Barclays Ctr       4 Avenue       
##  2 DeKalb Av                      4 Avenue       
##  3 Pacific St                     4 Avenue       
##  4 Grand Central                  42nd St Shuttle
##  5 34th St                        6 Avenue       
##  6 47-50th Sts Rockefeller Center 6 Avenue       
##  7 Church Av                      6 Avenue       
##  8 21st St                        63rd Street    
##  9 Lexington Av                   63rd Street    
## 10 Roosevelt Island               63rd Street    
## # … with 74 more rows</code></pre>
<p>There are 84 ADA compliant stations.</p>
<p>We can use the following code to find the proportion of station
entrances/exits without vending allow entrance:</p>
<pre class="r"><code>subway %&gt;% 
  filter(vending == &quot;NO&quot;) %&gt;% 
  pull(entry) %&gt;% 
  mean</code></pre>
<pre><code>## [1] 0.3770492</code></pre>
<p>The proportion is 0.377.</p>
<p>We can use the following code to find how many stations serve the A
train and of the stations that serve the A train, how many are ADA
compliant:</p>
<pre class="r"><code>subway %&gt;% 
  pivot_longer(
    route1:route11,
    names_to = &quot;route_num&quot;,
    values_to = &quot;route&quot;) %&gt;% 
  filter(route == &quot;A&quot;) %&gt;% 
  select(station_name, line) %&gt;% 
  distinct</code></pre>
<pre><code>## # A tibble: 60 × 2
##    station_name                  line           
##    &lt;chr&gt;                         &lt;chr&gt;          
##  1 Times Square                  42nd St Shuttle
##  2 125th St                      8 Avenue       
##  3 145th St                      8 Avenue       
##  4 14th St                       8 Avenue       
##  5 168th St - Washington Heights 8 Avenue       
##  6 175th St                      8 Avenue       
##  7 181st St                      8 Avenue       
##  8 190th St                      8 Avenue       
##  9 34th St                       8 Avenue       
## 10 42nd St                       8 Avenue       
## # … with 50 more rows</code></pre>
<pre class="r"><code>subway %&gt;% 
  pivot_longer(
    route1:route11,
    names_to = &quot;route_num&quot;,
    values_to = &quot;route&quot;) %&gt;% 
  filter(route == &quot;A&quot;, ada == TRUE) %&gt;% 
  select(station_name, line) %&gt;% 
  distinct</code></pre>
<pre><code>## # A tibble: 17 × 2
##    station_name                  line            
##    &lt;chr&gt;                         &lt;chr&gt;           
##  1 14th St                       8 Avenue        
##  2 168th St - Washington Heights 8 Avenue        
##  3 175th St                      8 Avenue        
##  4 34th St                       8 Avenue        
##  5 42nd St                       8 Avenue        
##  6 59th St                       8 Avenue        
##  7 Inwood - 207th St             8 Avenue        
##  8 West 4th St                   8 Avenue        
##  9 World Trade Center            8 Avenue        
## 10 Times Square-42nd St          Broadway        
## 11 59th St-Columbus Circle       Broadway-7th Ave
## 12 Times Square                  Broadway-7th Ave
## 13 8th Av                        Canarsie        
## 14 Franklin Av                   Franklin        
## 15 Euclid Av                     Fulton          
## 16 Franklin Av                   Fulton          
## 17 Howard Beach                  Rockaway</code></pre>
<p>There are 60 stations that serve the A train and of those, 17 are ADA
compliant.</p>
</div>
<div id="example-2-mr.-trash-wheel-dataset-cleaning"
class="section level3">
<h3>Example 2: Mr. Trash Wheel Dataset cleaning:</h3>
<p>Let’s start by reading and cleaning the Mr. Trash Wheel and Professor
Trash Wheel datasets.</p>
<p><strong>Make sure to import the correct sheet from the Excel
file:</strong></p>
<pre class="r"><code>trash = read_excel(&quot;./data/Trash_Wheel.xlsx&quot;, sheet = &quot;Mr. Trash Wheel&quot;, range = &quot;A2:N549&quot;) %&gt;% 
  janitor::clean_names() %&gt;% 
  drop_na(dumpster) %&gt;% 
  mutate(sports_balls = as.integer(round(sports_balls))) %&gt;% 
  mutate(ID = &quot;A&quot;)</code></pre>
<pre class="r"><code>professor = read_excel(&quot;./data/Trash_Wheel.xlsx&quot;, sheet = &quot;Professor Trash Wheel&quot;, range = &quot;A2:M96&quot;) %&gt;% 
  janitor::clean_names() %&gt;% 
  drop_na(dumpster) %&gt;% 
  mutate(ID = &quot;B&quot;)</code></pre>
<p>Next we will combine both datasets into one dataset.</p>
<pre class="r"><code>combo = merge(x = trash, y = professor, all = TRUE) %&gt;% 
  select(ID, everything())</code></pre>
<p>The new and combined dataset is a full merge and has 641 observations
(<code>nrow(combo)</code>) and 15 variables (<code>ncol(combo)</code>).
All the variables exist in both sets except for the
<em>sports_balls</em> variable; it came from the <strong>trash</strong>
dataset. We can distinguish which observation is from which dataset by
the <em>ID</em> variable; an <em>ID</em> value equal to A is for the
<strong>trash</strong> dataset and an <em>ID</em> value of B is for the
<strong>professor</strong> dataset.</p>
<p>To find the total weight of trash collected by Professor Trash Wheel,
we can use the following code:
<code>sum(subset(combo, ID == "B")$weight_tons)</code>, which gives us
the sum of the <em>weight_tons</em> variable restricted to the
observations from the <strong>Professor</strong> dataset, identified by
<em>ID = B</em>. The answer is 190.12 tons.</p>
<p>To find the total number of sports balls collected by Mr. Trash Wheel
in 2020, we can use the following code:
<code>sum(subset(combo, ID == "A" &amp; year == "2020")$sports_balls)</code>,
which gives us the sum of the <em>sports_balls</em> variable restricted
to the observations from the <strong>Trash</strong> dataset, identified
by <em>ID = A</em>, and only in the year 2020. The answer is 856 sports
balls.</p>
</div>
<div id="example-3-merging-multiple-datasets" class="section level3">
<h3>Example 3: Merging multiple datasets:</h3>
<p>Clean up the pols-month file:</p>
<pre class="r"><code>pols = read_csv(&quot;./data/fivethirtyeight_datasets/pols-month.csv&quot;) %&gt;% 
    janitor::clean_names() %&gt;% 
    separate(mon, sep = &quot;-&quot;, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) %&gt;%
    mutate(month = month.name[as.numeric(month)]) %&gt;% 
    mutate(year = as.numeric(year)) %&gt;% 
    mutate(month = factor(month, levels = month.name)) %&gt;% 
    mutate(president = case_when(prez_gop == 1 ~ &quot;gop&quot;, TRUE ~ &quot;dem&quot;)) %&gt;% 
    select(-day, -prez_gop, -prez_dem) %&gt;% 
    arrange(year, month)</code></pre>
<pre><code>## Rows: 822 Columns: 9
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
## date (1): mon
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<p>Clean up the snp file:</p>
<pre class="r"><code>snp = read_csv(&quot;./data/fivethirtyeight_datasets/snp.csv&quot;) %&gt;% 
    janitor::clean_names() %&gt;% 
    separate(date, sep = &quot;/&quot;, into = c(&quot;month&quot;, &quot;day&quot;, &quot;year&quot;)) %&gt;% 
    mutate(month = month.name[as.numeric(month)]) %&gt;% 
    mutate(century = case_when(year &lt; 16 ~ 2000, TRUE ~ 1900)) %&gt;% 
    mutate(year = as.numeric(year)) %&gt;% 
    mutate(year = year + century) %&gt;% 
    select(year, month, close) %&gt;% 
    mutate(month = factor(month, levels = month.name)) %&gt;% 
    arrange(year, month) </code></pre>
<pre><code>## Rows: 787 Columns: 2
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr (1): date
## dbl (1): close
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<p>Now tidy the unemployment file:</p>
<pre class="r"><code>unemploy = read_csv(&quot;./data/fivethirtyeight_datasets/unemployment.csv&quot;) %&gt;% 
    janitor::clean_names() %&gt;% 
    pivot_longer(
      jan:dec,
      names_to = &quot;month&quot;,
      values_to = &quot;percent_unemploy&quot;) %&gt;% 
    mutate(month = as.factor(month)) %&gt;% 
    mutate(month = month.name[as.numeric(month)]) %&gt;% 
    mutate(month = as.factor(month)) %&gt;% 
    mutate(month = factor(month, levels = month.name)) %&gt;% 
    arrange(year, month)</code></pre>
<pre><code>## Rows: 68 Columns: 13
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<p>Now we will create a merged dataset in 2 steps.</p>
<p>First we will merge <code>snp</code> into <code>pols</code>:</p>
<pre class="r"><code>first = left_join(pols, snp, by = c(&quot;year&quot;, &quot;month&quot;))</code></pre>
<p>Then we will merge the <code>unemploy</code> file into this new
merged <code>first</code> file:</p>
<pre class="r"><code>total = left_join(first, unemploy, by = c(&quot;year&quot;, &quot;month&quot;))</code></pre>
<p>The <strong>snp</strong> dataset has just 3 variables: the
<em>close</em> variable, which was untouched, and then <em>month</em>
and <em>year</em> that we created by separating the date. The
<strong>pols</strong> dataset has the same <em>month</em> and
<em>year</em> that we made like in the <strong>snp</strong> dataset. It
also has many of the original variables, as well as a new variable
called <em>president</em> which was created logically based off the
<em>prez_dem</em> and <em>prez_gop</em> variables. The
<strong>unemploy</strong> dataset has also 3 variables, with the
<em>month</em> and <em>year</em> variables made by separating the date
and the <em>percent_unemploy</em> variable made by the
<code>pivot_longer</code> function.</p>
<p>The final dataset <strong>total</strong> has 822 observations
(<code>nrow(total</code>) and 11 variables (<code>ncol(total</code>). We
can find the range of years with the following code:
<code>range(total$year)</code>, which is from 1947 to 2015. The key
variables are <em>year</em> and <em>month</em> which was present in all
3 datafiles and was used to perform all the merges.</p>
</div>
</div>
<div id="exploratory-data-analysis-eda" class="section level2">
<h2>Exploratory Data Analysis (EDA):</h2>
<div
id="noaa-national-climatic-data-center-for-all-weather-stations-in-new-york-state"
class="section level3">
<h3>NOAA National Climatic Data Center for all weather stations in New
York State:</h3>
<p>Let’s load the data:</p>
<pre class="r"><code>data(&quot;ny_noaa&quot;)

ny_noaa</code></pre>
<pre><code>## # A tibble: 2,595,176 × 7
##    id          date        prcp  snow  snwd tmax  tmin 
##    &lt;chr&gt;       &lt;date&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;
##  1 US1NYAB0001 2007-11-01    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  2 US1NYAB0001 2007-11-02    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  3 US1NYAB0001 2007-11-03    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  4 US1NYAB0001 2007-11-04    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  5 US1NYAB0001 2007-11-05    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  6 US1NYAB0001 2007-11-06    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  7 US1NYAB0001 2007-11-07    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  8 US1NYAB0001 2007-11-08    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
##  9 US1NYAB0001 2007-11-09    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
## 10 US1NYAB0001 2007-11-10    NA    NA    NA &lt;NA&gt;  &lt;NA&gt; 
## # … with 2,595,166 more rows</code></pre>
<p>The dataset has 7 variables (<code>ncol(ny_noaa)</code>) and
2,595,176 observations (<code>nrow(ny_noaa)</code>). It has a
combination of integer and character variables, with also a date
variable. The variables that make up the dataset are an ID variable of
the weather station, date of observation, precipitation (mm), snowfall
(mm), snow depth (mm), and the maximum and minimum temperatures in
Celsius. There is a large number of missing data because each weather
station may collect only a subset of these variables, so the dataset has
observations with missing data.</p>
<p>Let’s now do some data cleaning of this dataset:</p>
<pre class="r"><code>ny_noaa_clean &lt;- ny_noaa %&gt;% 
  janitor::clean_names() %&gt;%
  separate(date, sep = &quot;-&quot;, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) %&gt;%
  mutate_at(c(2:9), as.numeric) %&gt;% 
  mutate(prcp = prcp/10) %&gt;% 
  mutate(tmin = tmin/10, tmax = tmax/10) %&gt;% 
  mutate(month = month.name[as.numeric(month)])
ny_noaa_clean</code></pre>
<pre><code>## # A tibble: 2,595,176 × 9
##    id           year month      day  prcp  snow  snwd  tmax  tmin
##    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 US1NYAB0001  2007 November     1    NA    NA    NA    NA    NA
##  2 US1NYAB0001  2007 November     2    NA    NA    NA    NA    NA
##  3 US1NYAB0001  2007 November     3    NA    NA    NA    NA    NA
##  4 US1NYAB0001  2007 November     4    NA    NA    NA    NA    NA
##  5 US1NYAB0001  2007 November     5    NA    NA    NA    NA    NA
##  6 US1NYAB0001  2007 November     6    NA    NA    NA    NA    NA
##  7 US1NYAB0001  2007 November     7    NA    NA    NA    NA    NA
##  8 US1NYAB0001  2007 November     8    NA    NA    NA    NA    NA
##  9 US1NYAB0001  2007 November     9    NA    NA    NA    NA    NA
## 10 US1NYAB0001  2007 November    10    NA    NA    NA    NA    NA
## # … with 2,595,166 more rows</code></pre>
<p>We cleaned up the data by cleaning the names, separating the variable
for date of observation into the year, month, and day, converting all
the variables except the ID into a numeric variable, and converting the
<code>prcp</code>, <code>tmix</code>, and <code>tmax</code> variables
from it’s tenths value to it’s whole value by dividing by 10.</p>
<p>Now let’s make a two-panel plot showing the average max temperature
in January and in July in each station across years.</p>
<pre class="r"><code>avgtmax &lt;- ny_noaa_clean %&gt;% 
  filter(
    month %in% c(&quot;January&quot;, &quot;July&quot;)
  ) %&gt;% 
  drop_na(tmax) %&gt;% 
  group_by(year, id, month) %&gt;% 
  summarize(
    avg_tmax = mean(tmax, na.rm = TRUE)
  ) </code></pre>
<pre><code>## `summarise()` has grouped output by &#39;year&#39;, &#39;id&#39;. You can override using the
## `.groups` argument.</code></pre>
<pre class="r"><code>ggplot(avgtmax, (aes(x = year, y = avg_tmax, color = id))) +
  geom_point() +
  geom_line() +
  labs(
    title = &quot;Average maximum temperature vs. year by weather station in January and July&quot;,
    x = &quot;Year&quot;,
    y = &quot;Average maximum temperature (C)&quot;
  ) +
  facet_grid(. ~ month) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="tidy_data_files/figure-html/two_plot-1.png" width="90%" /></p>
<p>The overall trends in these graphs are that in January, the average
maximum temperature from 1980 to 2010 was between -10 and 10 degrees
Celsius. We have a few outliers at about 11, -9, -13, and -12. In July,
the average maximum temperature from 1980 to 2010 was between 20 and 35
degrees Celsius. Some of the outliers were 14, 18, 19, and 36.</p>
<p>Now let’s make a two-panel plot showing (i) tmax vs tmin for the full
dataset and (ii) the distribution of snowfall values greater than 0 and
less than 100 separately by year</p>
<pre class="r"><code>temp_plot = 
ny_noaa_clean %&gt;% 
ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
    labs(
    title = &quot;Minimum and maximum temperatures&quot;,
    x = &quot;Minimum temperature (C)&quot;,
    y = &quot;Maximum temperature (C)&quot;
    )


snow &lt;- ny_noaa_clean %&gt;% 
  filter(snow &gt; 0 &amp; snow &lt; 100) %&gt;%
  mutate(snow = as.numeric(snow)) %&gt;%
  mutate(year = as.factor(year))

snow_plot =   
ggplot(snow, aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    title = &quot;Snowfall values by year&quot;,
    x = &quot;Snowfall (mm)&quot;,
    y = &quot;Year&quot;
    )

temp_plot + snow_plot</code></pre>
<pre><code>## Picking joint bandwidth of 3.76</code></pre>
<p><img src="tidy_data_files/figure-html/diff_two_plot-1.png" width="90%" /></p>
<p>In these plots, we see that there is large number of days in which
the maximum temperature and minimum temperatures were between 15 for
tmin and 30 for tmax and -15 for tmin and -5 for tmax. For the snowfall
plot, from 1981 to 2010, most of the days with snowfall have a value
between 0 and 30 mm. There is also another large set of days that had
snowfall values between 40 and 60 mm and another between 70 and 80
mm.</p>
</div>
</div>
<div id="iteration-coding" class="section level2">
<h2>Iteration coding:</h2>
<div id="homicide-data" class="section level3">
<h3>Homicide data:</h3>
<p>The Washington Post has gathered data on homicides in 50 large U.S.
cities and that is the data used in this analysis.</p>
<pre class="r"><code>homicides = read.csv(&quot;./data/homicide-data.csv&quot;)</code></pre>
<p>The raw dataset <code>homicides</code> contains 12 variables and
52179 observations. It describes information on homicides from 50 large
US cities. Variables of interest include the case ID, the date of the
incident, information on the victim (name, race, age and sex),
information on the location of the homicide, and the disposition of the
case.</p>
<p>We will create a new variable <code>city_state</code>.</p>
<pre class="r"><code>new_homi &lt;- homicides %&gt;%
  unite(&quot;city_state&quot;, city:state, sep = &quot;, &quot;, remove = FALSE)</code></pre>
<p>We will first look at the total number of homicides in a city and
then at the number of homicides that are unsolved in a city.</p>
<pre class="r"><code>new_homi %&gt;% 
  group_by(city) %&gt;%
  summarize(count = n()) %&gt;%
  knitr::kable(digits = 1)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">city</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Albuquerque</td>
<td align="right">378</td>
</tr>
<tr class="even">
<td align="left">Atlanta</td>
<td align="right">973</td>
</tr>
<tr class="odd">
<td align="left">Baltimore</td>
<td align="right">2827</td>
</tr>
<tr class="even">
<td align="left">Baton Rouge</td>
<td align="right">424</td>
</tr>
<tr class="odd">
<td align="left">Birmingham</td>
<td align="right">800</td>
</tr>
<tr class="even">
<td align="left">Boston</td>
<td align="right">614</td>
</tr>
<tr class="odd">
<td align="left">Buffalo</td>
<td align="right">521</td>
</tr>
<tr class="even">
<td align="left">Charlotte</td>
<td align="right">687</td>
</tr>
<tr class="odd">
<td align="left">Chicago</td>
<td align="right">5535</td>
</tr>
<tr class="even">
<td align="left">Cincinnati</td>
<td align="right">694</td>
</tr>
<tr class="odd">
<td align="left">Columbus</td>
<td align="right">1084</td>
</tr>
<tr class="even">
<td align="left">Dallas</td>
<td align="right">1567</td>
</tr>
<tr class="odd">
<td align="left">Denver</td>
<td align="right">312</td>
</tr>
<tr class="even">
<td align="left">Detroit</td>
<td align="right">2519</td>
</tr>
<tr class="odd">
<td align="left">Durham</td>
<td align="right">276</td>
</tr>
<tr class="even">
<td align="left">Fort Worth</td>
<td align="right">549</td>
</tr>
<tr class="odd">
<td align="left">Fresno</td>
<td align="right">487</td>
</tr>
<tr class="even">
<td align="left">Houston</td>
<td align="right">2942</td>
</tr>
<tr class="odd">
<td align="left">Indianapolis</td>
<td align="right">1322</td>
</tr>
<tr class="even">
<td align="left">Jacksonville</td>
<td align="right">1168</td>
</tr>
<tr class="odd">
<td align="left">Kansas City</td>
<td align="right">1190</td>
</tr>
<tr class="even">
<td align="left">Las Vegas</td>
<td align="right">1381</td>
</tr>
<tr class="odd">
<td align="left">Long Beach</td>
<td align="right">378</td>
</tr>
<tr class="even">
<td align="left">Los Angeles</td>
<td align="right">2257</td>
</tr>
<tr class="odd">
<td align="left">Louisville</td>
<td align="right">576</td>
</tr>
<tr class="even">
<td align="left">Memphis</td>
<td align="right">1514</td>
</tr>
<tr class="odd">
<td align="left">Miami</td>
<td align="right">744</td>
</tr>
<tr class="even">
<td align="left">Milwaukee</td>
<td align="right">1115</td>
</tr>
<tr class="odd">
<td align="left">Minneapolis</td>
<td align="right">366</td>
</tr>
<tr class="even">
<td align="left">Nashville</td>
<td align="right">767</td>
</tr>
<tr class="odd">
<td align="left">New Orleans</td>
<td align="right">1434</td>
</tr>
<tr class="even">
<td align="left">New York</td>
<td align="right">627</td>
</tr>
<tr class="odd">
<td align="left">Oakland</td>
<td align="right">947</td>
</tr>
<tr class="even">
<td align="left">Oklahoma City</td>
<td align="right">672</td>
</tr>
<tr class="odd">
<td align="left">Omaha</td>
<td align="right">409</td>
</tr>
<tr class="even">
<td align="left">Philadelphia</td>
<td align="right">3037</td>
</tr>
<tr class="odd">
<td align="left">Phoenix</td>
<td align="right">914</td>
</tr>
<tr class="even">
<td align="left">Pittsburgh</td>
<td align="right">631</td>
</tr>
<tr class="odd">
<td align="left">Richmond</td>
<td align="right">429</td>
</tr>
<tr class="even">
<td align="left">Sacramento</td>
<td align="right">376</td>
</tr>
<tr class="odd">
<td align="left">San Antonio</td>
<td align="right">833</td>
</tr>
<tr class="even">
<td align="left">San Bernardino</td>
<td align="right">275</td>
</tr>
<tr class="odd">
<td align="left">San Diego</td>
<td align="right">461</td>
</tr>
<tr class="even">
<td align="left">San Francisco</td>
<td align="right">663</td>
</tr>
<tr class="odd">
<td align="left">Savannah</td>
<td align="right">246</td>
</tr>
<tr class="even">
<td align="left">St. Louis</td>
<td align="right">1677</td>
</tr>
<tr class="odd">
<td align="left">Stockton</td>
<td align="right">444</td>
</tr>
<tr class="even">
<td align="left">Tampa</td>
<td align="right">208</td>
</tr>
<tr class="odd">
<td align="left">Tulsa</td>
<td align="right">584</td>
</tr>
<tr class="even">
<td align="left">Washington</td>
<td align="right">1345</td>
</tr>
</tbody>
</table>
<pre class="r"><code>new_homi %&gt;% 
  filter(disposition %in% c(&quot;Closed without arrest&quot;, &quot;Open/No arrest&quot;)) %&gt;%
  group_by(city) %&gt;%
  summarize(count = n()) %&gt;%
  knitr::kable(digits = 1)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">city</th>
<th align="right">count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Albuquerque</td>
<td align="right">146</td>
</tr>
<tr class="even">
<td align="left">Atlanta</td>
<td align="right">373</td>
</tr>
<tr class="odd">
<td align="left">Baltimore</td>
<td align="right">1825</td>
</tr>
<tr class="even">
<td align="left">Baton Rouge</td>
<td align="right">196</td>
</tr>
<tr class="odd">
<td align="left">Birmingham</td>
<td align="right">347</td>
</tr>
<tr class="even">
<td align="left">Boston</td>
<td align="right">310</td>
</tr>
<tr class="odd">
<td align="left">Buffalo</td>
<td align="right">319</td>
</tr>
<tr class="even">
<td align="left">Charlotte</td>
<td align="right">206</td>
</tr>
<tr class="odd">
<td align="left">Chicago</td>
<td align="right">4073</td>
</tr>
<tr class="even">
<td align="left">Cincinnati</td>
<td align="right">309</td>
</tr>
<tr class="odd">
<td align="left">Columbus</td>
<td align="right">575</td>
</tr>
<tr class="even">
<td align="left">Dallas</td>
<td align="right">754</td>
</tr>
<tr class="odd">
<td align="left">Denver</td>
<td align="right">169</td>
</tr>
<tr class="even">
<td align="left">Detroit</td>
<td align="right">1482</td>
</tr>
<tr class="odd">
<td align="left">Durham</td>
<td align="right">101</td>
</tr>
<tr class="even">
<td align="left">Fort Worth</td>
<td align="right">255</td>
</tr>
<tr class="odd">
<td align="left">Fresno</td>
<td align="right">169</td>
</tr>
<tr class="even">
<td align="left">Houston</td>
<td align="right">1493</td>
</tr>
<tr class="odd">
<td align="left">Indianapolis</td>
<td align="right">594</td>
</tr>
<tr class="even">
<td align="left">Jacksonville</td>
<td align="right">597</td>
</tr>
<tr class="odd">
<td align="left">Kansas City</td>
<td align="right">486</td>
</tr>
<tr class="even">
<td align="left">Las Vegas</td>
<td align="right">572</td>
</tr>
<tr class="odd">
<td align="left">Long Beach</td>
<td align="right">156</td>
</tr>
<tr class="even">
<td align="left">Los Angeles</td>
<td align="right">1106</td>
</tr>
<tr class="odd">
<td align="left">Louisville</td>
<td align="right">261</td>
</tr>
<tr class="even">
<td align="left">Memphis</td>
<td align="right">483</td>
</tr>
<tr class="odd">
<td align="left">Miami</td>
<td align="right">450</td>
</tr>
<tr class="even">
<td align="left">Milwaukee</td>
<td align="right">403</td>
</tr>
<tr class="odd">
<td align="left">Minneapolis</td>
<td align="right">187</td>
</tr>
<tr class="even">
<td align="left">Nashville</td>
<td align="right">278</td>
</tr>
<tr class="odd">
<td align="left">New Orleans</td>
<td align="right">930</td>
</tr>
<tr class="even">
<td align="left">New York</td>
<td align="right">243</td>
</tr>
<tr class="odd">
<td align="left">Oakland</td>
<td align="right">508</td>
</tr>
<tr class="even">
<td align="left">Oklahoma City</td>
<td align="right">326</td>
</tr>
<tr class="odd">
<td align="left">Omaha</td>
<td align="right">169</td>
</tr>
<tr class="even">
<td align="left">Philadelphia</td>
<td align="right">1360</td>
</tr>
<tr class="odd">
<td align="left">Phoenix</td>
<td align="right">504</td>
</tr>
<tr class="even">
<td align="left">Pittsburgh</td>
<td align="right">337</td>
</tr>
<tr class="odd">
<td align="left">Richmond</td>
<td align="right">113</td>
</tr>
<tr class="even">
<td align="left">Sacramento</td>
<td align="right">139</td>
</tr>
<tr class="odd">
<td align="left">San Antonio</td>
<td align="right">357</td>
</tr>
<tr class="even">
<td align="left">San Bernardino</td>
<td align="right">170</td>
</tr>
<tr class="odd">
<td align="left">San Diego</td>
<td align="right">175</td>
</tr>
<tr class="even">
<td align="left">San Francisco</td>
<td align="right">336</td>
</tr>
<tr class="odd">
<td align="left">Savannah</td>
<td align="right">115</td>
</tr>
<tr class="even">
<td align="left">St. Louis</td>
<td align="right">905</td>
</tr>
<tr class="odd">
<td align="left">Stockton</td>
<td align="right">266</td>
</tr>
<tr class="even">
<td align="left">Tampa</td>
<td align="right">95</td>
</tr>
<tr class="odd">
<td align="left">Tulsa</td>
<td align="right">193</td>
</tr>
<tr class="even">
<td align="left">Washington</td>
<td align="right">589</td>
</tr>
</tbody>
</table>
<p>Let’s focus on crime in just Baltimore, MD:</p>
<pre class="r"><code>new_homi %&gt;%
  filter(city_state == &quot;Baltimore, MD&quot;) %&gt;%
  summarize(count = n())</code></pre>
<pre><code>##   count
## 1  2827</code></pre>
<pre class="r"><code>new_homi %&gt;%
  filter(city_state == &quot;Baltimore, MD&quot;) %&gt;%
  filter(disposition %in% c(&quot;Closed without arrest&quot;, &quot;Open/No arrest&quot;)) %&gt;%
  summarize(count = n())</code></pre>
<pre><code>##   count
## 1  1825</code></pre>
<p>From the above code, we see that in the city of Baltimore, MD, there
were a total of 2827 homicides and of those homicides, 1825 were
unsolved.</p>
<p>Let’s use the <code>prop.test</code> function on just Baltimore, MD
to estimate the proportion of homicides that are unsolved and its
confidence interval.</p>
<pre class="r"><code>Balt_test &lt;- prop.test(1825, 2827) %&gt;%
  broom::tidy() %&gt;%
  select(estimate, starts_with(&quot;conf&quot;))</code></pre>
<p>The proportion of homicides that are unsolved is 0.646 with a
confidence interval of 0.628 and 0.663.</p>
<p>Now we will create a function to do this for all the cities in the
dataset:</p>
<pre class="r"><code>homi_nest =
  new_homi %&gt;% 
  relocate(city_state) %&gt;% 
  nest(data = uid:disposition)

prop = function(df) {

  data_result = df %&gt;% 
    summarize(total = n(),
              unsolved = sum(disposition %in% c(&quot;Closed without arrest&quot;, &quot;Open/No arrest&quot;)))
    
  prop_results = 
    prop.test(x = data_result %&gt;% pull(unsolved), 
              n = data_result %&gt;% pull(total))

  return(prop_results)
}


final &lt;- homi_nest %&gt;% 
  mutate(results = map(data, prop),
         estimates = map(results, broom::tidy)) %&gt;% 
  select(city_state, estimates) %&gt;% 
  unnest(estimates) %&gt;%
  select(city_state, estimate, starts_with(&quot;conf&quot;))


final %&gt;% 
  filter(city_state != &quot;Tulsa, AL&quot;) %&gt;% 
  mutate(city_state = fct_reorder(city_state, estimate)) %&gt;% 
  ggplot(aes(x = city_state, y = estimate)) + 
    geom_line() +
    geom_point()+
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=.2,
                 position=position_dodge(0.05))+
    theme(axis.text.x = element_text(angle = 90))</code></pre>
<pre><code>## `geom_line()`: Each group consists of only one observation.
## ℹ Do you need to adjust the group aesthetic?</code></pre>
<p><img src="tidy_data_files/figure-html/unnamed-chunk-10-1.png" width="90%" /></p>
<p>The above plot shows the proportion of unsolved homicides in each
city with its corresponding confidence interval. There was an error in
the data where Tulsa, AL was incorrected entered for Tulsa, OK. Instead
of completely removing it from the dataset, I filtered it out of the
plot.</p>
</div>
</div>
<div id="logistic-regression-coding" class="section level2">
<h2>Logistic Regression coding:</h2>
<div id="homicide-data-1" class="section level3">
<h3>Homicide data:</h3>
<p>We will start by loading the homicide data.</p>
<pre class="r"><code>homicides_two = read.csv(&quot;./data/homicide-data.csv&quot;)</code></pre>
<p>Next we will do some data cleaning as below:</p>
<pre class="r"><code>homicides_two =   
  homicides_two %&gt;%
  unite(&quot;city_state&quot;, city:state, sep = &quot;, &quot;, remove = FALSE) %&gt;% 
  mutate(solved = as.numeric(disposition == &quot;Closed by arrest&quot;)) %&gt;%
  filter(!(city %in% c(&quot;Dallas&quot;, &quot;Phoenix&quot;, &quot;Kansas City&quot;))) %&gt;%
  filter(city_state != &quot;Tulsa, AL&quot;) %&gt;%
  mutate(victim_age = as.numeric(victim_age)) %&gt;%
  filter(victim_race %in% c(&quot;White&quot;, &quot;Black&quot;)) %&gt;%
  mutate(victim_race = fct_relevel(victim_race, &quot;White&quot;)) %&gt;%
  filter(victim_sex != &quot;Unknown&quot;)</code></pre>
<p>Now for just the city of Baltimore, we will fit a logistic model with
homicide being solved as the outcome and with victim age, sex, and race
as predictors.</p>
<pre class="r"><code>Balt =
  homicides_two %&gt;%
  filter(city == &quot;Baltimore&quot;) %&gt;%
  select(solved, victim_age, victim_race, victim_sex)

fit_balt = 
  Balt %&gt;% 
  glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

fit_balt %&gt;% 
  broom::tidy() %&gt;%
  mutate(lower_CI = exp(estimate - 1.96*std.error),
         upper_CI = exp(estimate + 1.96*std.error),
         OR = exp(estimate)) %&gt;%
  select(term, OR, lower_CI, upper_CI) %&gt;% 
  knitr::kable(digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">OR</th>
<th align="right">lower_CI</th>
<th align="right">upper_CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">3.164</td>
<td align="right">1.989</td>
<td align="right">5.031</td>
</tr>
<tr class="even">
<td align="left">victim_age</td>
<td align="right">0.993</td>
<td align="right">0.987</td>
<td align="right">1.000</td>
</tr>
<tr class="odd">
<td align="left">victim_raceBlack</td>
<td align="right">0.431</td>
<td align="right">0.306</td>
<td align="right">0.607</td>
</tr>
<tr class="even">
<td align="left">victim_sexMale</td>
<td align="right">0.426</td>
<td align="right">0.325</td>
<td align="right">0.558</td>
</tr>
</tbody>
</table>
<p>Let’s repeat this for all the cities and get the OR with their 95% CI
for solving homicides comparing males to females, adjusting for race and
age.</p>
<pre class="r"><code>homi_nest =
  homicides_two %&gt;% 
  select(city_state, solved, victim_age, victim_race, victim_sex) %&gt;% 
  relocate(city_state) %&gt;% 
  nest(data = solved:victim_sex)


fit_all = 
  homi_nest %&gt;% 
  mutate(
    models = map(.x = data, ~glm(solved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)) %&gt;% 
  select(-data, -models) %&gt;% 
  unnest(results) %&gt;% 
  mutate(lower_CI = exp(estimate - 1.96*std.error),
         upper_CI = exp(estimate + 1.96*std.error),
         OR = exp(estimate)) %&gt;%
  select(city_state, term, OR, lower_CI, upper_CI) %&gt;%
  filter(term == &quot;victim_sexMale&quot;)

fit_all %&gt;%
  knitr::kable(digits = 3)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">city_state</th>
<th align="left">term</th>
<th align="right">OR</th>
<th align="right">lower_CI</th>
<th align="right">upper_CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Albuquerque, NM</td>
<td align="left">victim_sexMale</td>
<td align="right">1.767</td>
<td align="right">0.831</td>
<td align="right">3.761</td>
</tr>
<tr class="even">
<td align="left">Atlanta, GA</td>
<td align="left">victim_sexMale</td>
<td align="right">1.000</td>
<td align="right">0.684</td>
<td align="right">1.463</td>
</tr>
<tr class="odd">
<td align="left">Baltimore, MD</td>
<td align="left">victim_sexMale</td>
<td align="right">0.426</td>
<td align="right">0.325</td>
<td align="right">0.558</td>
</tr>
<tr class="even">
<td align="left">Baton Rouge, LA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.381</td>
<td align="right">0.209</td>
<td align="right">0.695</td>
</tr>
<tr class="odd">
<td align="left">Birmingham, AL</td>
<td align="left">victim_sexMale</td>
<td align="right">0.870</td>
<td align="right">0.574</td>
<td align="right">1.318</td>
</tr>
<tr class="even">
<td align="left">Boston, MA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.667</td>
<td align="right">0.354</td>
<td align="right">1.260</td>
</tr>
<tr class="odd">
<td align="left">Buffalo, NY</td>
<td align="left">victim_sexMale</td>
<td align="right">0.521</td>
<td align="right">0.290</td>
<td align="right">0.935</td>
</tr>
<tr class="even">
<td align="left">Charlotte, NC</td>
<td align="left">victim_sexMale</td>
<td align="right">0.884</td>
<td align="right">0.557</td>
<td align="right">1.403</td>
</tr>
<tr class="odd">
<td align="left">Chicago, IL</td>
<td align="left">victim_sexMale</td>
<td align="right">0.410</td>
<td align="right">0.336</td>
<td align="right">0.501</td>
</tr>
<tr class="even">
<td align="left">Cincinnati, OH</td>
<td align="left">victim_sexMale</td>
<td align="right">0.400</td>
<td align="right">0.236</td>
<td align="right">0.677</td>
</tr>
<tr class="odd">
<td align="left">Columbus, OH</td>
<td align="left">victim_sexMale</td>
<td align="right">0.532</td>
<td align="right">0.378</td>
<td align="right">0.750</td>
</tr>
<tr class="even">
<td align="left">Denver, CO</td>
<td align="left">victim_sexMale</td>
<td align="right">0.479</td>
<td align="right">0.236</td>
<td align="right">0.971</td>
</tr>
<tr class="odd">
<td align="left">Detroit, MI</td>
<td align="left">victim_sexMale</td>
<td align="right">0.582</td>
<td align="right">0.462</td>
<td align="right">0.734</td>
</tr>
<tr class="even">
<td align="left">Durham, NC</td>
<td align="left">victim_sexMale</td>
<td align="right">0.812</td>
<td align="right">0.392</td>
<td align="right">1.683</td>
</tr>
<tr class="odd">
<td align="left">Fort Worth, TX</td>
<td align="left">victim_sexMale</td>
<td align="right">0.669</td>
<td align="right">0.397</td>
<td align="right">1.127</td>
</tr>
<tr class="even">
<td align="left">Fresno, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">1.335</td>
<td align="right">0.580</td>
<td align="right">3.071</td>
</tr>
<tr class="odd">
<td align="left">Houston, TX</td>
<td align="left">victim_sexMale</td>
<td align="right">0.711</td>
<td align="right">0.558</td>
<td align="right">0.907</td>
</tr>
<tr class="even">
<td align="left">Indianapolis, IN</td>
<td align="left">victim_sexMale</td>
<td align="right">0.919</td>
<td align="right">0.679</td>
<td align="right">1.242</td>
</tr>
<tr class="odd">
<td align="left">Jacksonville, FL</td>
<td align="left">victim_sexMale</td>
<td align="right">0.720</td>
<td align="right">0.537</td>
<td align="right">0.966</td>
</tr>
<tr class="even">
<td align="left">Las Vegas, NV</td>
<td align="left">victim_sexMale</td>
<td align="right">0.837</td>
<td align="right">0.608</td>
<td align="right">1.154</td>
</tr>
<tr class="odd">
<td align="left">Long Beach, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.410</td>
<td align="right">0.156</td>
<td align="right">1.082</td>
</tr>
<tr class="even">
<td align="left">Los Angeles, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.662</td>
<td align="right">0.458</td>
<td align="right">0.956</td>
</tr>
<tr class="odd">
<td align="left">Louisville, KY</td>
<td align="left">victim_sexMale</td>
<td align="right">0.491</td>
<td align="right">0.305</td>
<td align="right">0.790</td>
</tr>
<tr class="even">
<td align="left">Memphis, TN</td>
<td align="left">victim_sexMale</td>
<td align="right">0.723</td>
<td align="right">0.529</td>
<td align="right">0.988</td>
</tr>
<tr class="odd">
<td align="left">Miami, FL</td>
<td align="left">victim_sexMale</td>
<td align="right">0.515</td>
<td align="right">0.304</td>
<td align="right">0.872</td>
</tr>
<tr class="even">
<td align="left">Milwaukee, wI</td>
<td align="left">victim_sexMale</td>
<td align="right">0.727</td>
<td align="right">0.499</td>
<td align="right">1.060</td>
</tr>
<tr class="odd">
<td align="left">Minneapolis, MN</td>
<td align="left">victim_sexMale</td>
<td align="right">0.947</td>
<td align="right">0.478</td>
<td align="right">1.875</td>
</tr>
<tr class="even">
<td align="left">Nashville, TN</td>
<td align="left">victim_sexMale</td>
<td align="right">1.034</td>
<td align="right">0.685</td>
<td align="right">1.562</td>
</tr>
<tr class="odd">
<td align="left">New Orleans, LA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.585</td>
<td align="right">0.422</td>
<td align="right">0.811</td>
</tr>
<tr class="even">
<td align="left">New York, NY</td>
<td align="left">victim_sexMale</td>
<td align="right">0.262</td>
<td align="right">0.138</td>
<td align="right">0.499</td>
</tr>
<tr class="odd">
<td align="left">Oakland, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.563</td>
<td align="right">0.365</td>
<td align="right">0.868</td>
</tr>
<tr class="even">
<td align="left">Oklahoma City, OK</td>
<td align="left">victim_sexMale</td>
<td align="right">0.974</td>
<td align="right">0.624</td>
<td align="right">1.520</td>
</tr>
<tr class="odd">
<td align="left">Omaha, NE</td>
<td align="left">victim_sexMale</td>
<td align="right">0.382</td>
<td align="right">0.203</td>
<td align="right">0.721</td>
</tr>
<tr class="even">
<td align="left">Philadelphia, PA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.496</td>
<td align="right">0.378</td>
<td align="right">0.652</td>
</tr>
<tr class="odd">
<td align="left">Pittsburgh, PA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.431</td>
<td align="right">0.265</td>
<td align="right">0.700</td>
</tr>
<tr class="even">
<td align="left">Richmond, VA</td>
<td align="left">victim_sexMale</td>
<td align="right">1.006</td>
<td align="right">0.498</td>
<td align="right">2.033</td>
</tr>
<tr class="odd">
<td align="left">San Antonio, TX</td>
<td align="left">victim_sexMale</td>
<td align="right">0.705</td>
<td align="right">0.398</td>
<td align="right">1.249</td>
</tr>
<tr class="even">
<td align="left">Sacramento, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.669</td>
<td align="right">0.335</td>
<td align="right">1.337</td>
</tr>
<tr class="odd">
<td align="left">Savannah, GA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.867</td>
<td align="right">0.422</td>
<td align="right">1.780</td>
</tr>
<tr class="even">
<td align="left">San Bernardino, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.500</td>
<td align="right">0.171</td>
<td align="right">1.462</td>
</tr>
<tr class="odd">
<td align="left">San Diego, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.413</td>
<td align="right">0.200</td>
<td align="right">0.855</td>
</tr>
<tr class="even">
<td align="left">San Francisco, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">0.608</td>
<td align="right">0.317</td>
<td align="right">1.165</td>
</tr>
<tr class="odd">
<td align="left">St. Louis, MO</td>
<td align="left">victim_sexMale</td>
<td align="right">0.703</td>
<td align="right">0.530</td>
<td align="right">0.932</td>
</tr>
<tr class="even">
<td align="left">Stockton, CA</td>
<td align="left">victim_sexMale</td>
<td align="right">1.352</td>
<td align="right">0.621</td>
<td align="right">2.942</td>
</tr>
<tr class="odd">
<td align="left">Tampa, FL</td>
<td align="left">victim_sexMale</td>
<td align="right">0.808</td>
<td align="right">0.348</td>
<td align="right">1.876</td>
</tr>
<tr class="even">
<td align="left">Tulsa, OK</td>
<td align="left">victim_sexMale</td>
<td align="right">0.976</td>
<td align="right">0.614</td>
<td align="right">1.552</td>
</tr>
<tr class="odd">
<td align="left">Washington, DC</td>
<td align="left">victim_sexMale</td>
<td align="right">0.691</td>
<td align="right">0.469</td>
<td align="right">1.018</td>
</tr>
</tbody>
</table>
<p>Now we will make a plot showing the OR and 95% CI for each city.</p>
<pre class="r"><code>fit_all %&gt;% 
  mutate(city_state = fct_reorder(city_state, OR)) %&gt;% 
  ggplot(aes(x = city_state, y = OR)) +        
    geom_point() +
    geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI)) +
    theme(axis.text.x = element_text(angle = 80, hjust = 1))</code></pre>
<p><img src="tidy_data_files/figure-html/unnamed-chunk-15-1.png" width="90%" /></p>
<p>From this plot, we can see the city with the lowest OR for solved
homicides comparing male victims to females while adjusting for race and
age is New York at 0.26 and the highest is Albuquerque at 1.77.</p>
<p><strong>The interpretations are as follows:</strong></p>
<p><strong>In New York City, the odds of solving a homicide case for a
male is 0.26 times the odds of solving a homicide case for a female,
adjusting for race and age. In Albuquerque, the odds of solving a
homicide case for a male is 1.77 times the odds of solving a homicide
case for a female, adjusting for race and age.</strong></p>
<p><strong>However, the confidence interval for Albuquerque includes the
null value of 1 making the OR not statistically significant whereas 1 is
not in the confidence interval for New York, which means the OR is
statistically significant. We would need to double check with the
p-values.</strong></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
