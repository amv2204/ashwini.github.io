---
title: "R Programming Code"
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

## Preparation:

We'll start by having a code chunk in the beginning that loads all the packages we will need and set up formatting for visualizations:

```{r setup, include = FALSE}
library(tidyverse)
library(readxl)
library(p8105.datasets)
library(hexbin)
library(ggridges)
library(patchwork)
library(mgcv)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Data cleaning and Tidying:

### NYC Transit dataset cleaning:

This dataset contains information related to each entrance and exit for each subway station in NYC.

We will start by reading in the datafile using the `readr` function from the `tidyverse` package and cleaning the data by using the `clean_names` function from the `janitor` package. We will also retain certain variables and convert the entry variable from a character variable to a logical variable. 

```{r subway}
subway = read_csv("./data/Subway.csv") %>% 
    janitor::clean_names() %>% 
    select(line, station_name, station_latitude, station_longitude, 
      starts_with("route"), entry, exit_only, vending, entrance_type, ada) %>% 
    mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) %>% 
    mutate(route8 = as.character(route8)) %>%  
    mutate(route9 = as.character(route9)) %>% 
    mutate(route10 = as.character(route10)) %>% 
    mutate(route11 = as.character(route11)) 

subway

```

This dataset contains 20 columns and 1868 rows It has the 20 variables that we selected it to keep. We imported the file, used the `clean_names` function to do a quick clean. Then we selected what variables we wanted to keep. Some of the route variables were in `dbl` format instead of `chr` like most of the route variables so we changed that. And lastly we turned the entry variable from character into a logical variable. 

This data is not tidy because the route variables should be converted from a wide to long format.

We can use the following code to find the number of distinct stations:

```{r}
subway %>% 
  select(station_name, line) %>% 
  distinct
```

There are 465 distinct stations.

We can use the following code to find the number of ADA compliant stations:

```{r}
subway %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

There are 84 ADA compliant stations.

We can use the following code to find the proportion of station entrances/exits without vending allow entrance:

```{r}
subway %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

The proportion is 0.377.

We can use the following code to find how many stations serve the A train and of the stations that serve the A train, how many are ADA compliant:

```{r}
subway %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct

subway %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

There are 60 stations that serve the A train and of those, 17 are ADA compliant.

### Mr. Trash Wheel Dataset cleaning:

Let's start by reading and cleaning the Mr. Trash Wheel and Professor Trash Wheel datasets. 

**Make sure to import the correct sheet from the Excel file:**

```{r Trashwheel}
trash = read_excel("./data/Trash_Wheel.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N549") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(sports_balls = as.integer(round(sports_balls))) %>% 
  mutate(ID = "A")
```

```{r Professor}
professor = read_excel("./data/Trash_Wheel.xlsx", sheet = "Professor Trash Wheel", range = "A2:M96") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(ID = "B")
```

Next we will combine both datasets into one dataset.

```{r combo}
combo = merge(x = trash, y = professor, all = TRUE) %>% 
  select(ID, everything())
```

The new and combined dataset is a full merge and has 641 observations (`nrow(combo)`) and 15 variables (`ncol(combo)`). All the variables exist in both sets except for the _sports_balls_ variable; it came from the **trash** dataset. We can distinguish which observation is from which dataset by the _ID_ variable; an _ID_ value equal to A is for the **trash** dataset and an _ID_ value of B is for the **professor** dataset. 

To find the total weight of trash collected by Professor Trash Wheel, we can use the following code: `sum(subset(combo, ID == "B")$weight_tons)`, which gives us the sum of the _weight_tons_ variable restricted to the observations from the **Professor** dataset, identified by _ID = B_. The answer is 190.12 tons.

To find the total number of sports balls collected by Mr. Trash Wheel in 2020, we can use the following code: `sum(subset(combo, ID == "A" & year == "2020")$sports_balls)`, which gives us the sum of the _sports_balls_ variable restricted to the observations from the **Trash** dataset, identified by _ID = A_, and only in the year 2020. The answer is 856 sports balls.


### Code to merge multiple datasets:

Clean up the pols-month file:

```{r pols}
pols = read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
    janitor::clean_names() %>% 
    separate(mon, sep = "-", into = c("year", "month", "day")) %>%
    mutate(month = month.name[as.numeric(month)]) %>% 
    mutate(year = as.numeric(year)) %>% 
    mutate(month = factor(month, levels = month.name)) %>% 
    mutate(president = case_when(prez_gop == 1 ~ "gop", TRUE ~ "dem")) %>% 
    select(-day, -prez_gop, -prez_dem) %>% 
    arrange(year, month)
```

Clean up the snp file:

```{r snp}
snp = read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
    janitor::clean_names() %>% 
    separate(date, sep = "/", into = c("month", "day", "year")) %>% 
    mutate(month = month.name[as.numeric(month)]) %>% 
    mutate(century = case_when(year < 16 ~ 2000, TRUE ~ 1900)) %>% 
    mutate(year = as.numeric(year)) %>% 
    mutate(year = year + century) %>% 
    select(year, month, close) %>% 
    mutate(month = factor(month, levels = month.name)) %>% 
    arrange(year, month) 
```

Now tidy the unemployment file:

```{r unemploy}
unemploy = read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      jan:dec,
      names_to = "month",
      values_to = "percent_unemploy") %>% 
    mutate(month = as.factor(month)) %>% 
    mutate(month = month.name[as.numeric(month)]) %>% 
    mutate(month = as.factor(month)) %>% 
    mutate(month = factor(month, levels = month.name)) %>% 
    arrange(year, month)
```


Now we will create a merged dataset in 2 steps.

First we will merge `snp` into `pols`:

```{r first_merge}
first = left_join(pols, snp, by = c("year", "month"))
```

Then we will merge the `unemploy` file into this new merged `first` file:

```{r total}
total = left_join(first, unemploy, by = c("year", "month"))
```

The **snp** dataset has just 3 variables: the _close_ variable, which was untouched, and then _month_ and _year_ that we created by separating the date. The **pols** dataset has the same _month_ and _year_ that we made like in the **snp** dataset. It also has many of the original variables, as well as a new variable called _president_ which was created logically based off the _prez_dem_ and _prez_gop_ variables. The **unemploy** dataset has also 3 variables, with the _month_ and _year_ variables made by separating the date and the _percent_unemploy_ variable made by the `pivot_longer` function. 

The final dataset **total** has 822 observations (`nrow(total`) and 11 variables (`ncol(total`). We can find the range of years with the following code: `range(total$year)`, which is from 1947 to 2015. The key variables are _year_ and _month_ which was present in all 3 datafiles and was used to perform all the merges. 

## Exploratory Data Analysis (EDA):

### NOAA National Climatic Data Center for all weather stations in New York State:

Let's load the data:

```{r load_nynoaa}
data("ny_noaa")

ny_noaa
```

The dataset has 7 variables (`ncol(ny_noaa)`) and 2,595,176 observations (`nrow(ny_noaa)`). It has a combination of integer and character variables, with also a date variable. The variables that make up the dataset are an ID variable of the weather station, date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures in Celsius. There is a large number of missing data because each weather station may collect only a subset of these variables, so the dataset has observations with missing data. 

Let's now do some data cleaning of this dataset:

```{r clean_nynoaa}
ny_noaa_clean <- ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) %>%
  mutate_at(c(2:9), as.numeric) %>% 
  mutate(prcp = prcp/10) %>% 
  mutate(tmin = tmin/10, tmax = tmax/10) %>% 
  mutate(month = month.name[as.numeric(month)])
ny_noaa_clean
```

We cleaned up the data by cleaning the names, separating the variable for date of observation into the year, month, and day, converting all the variables except the ID into a numeric variable, and converting the `prcp`, `tmix`, and `tmax` variables from it's tenths value to it's whole value by dividing by 10.  

Now let's make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r two_plot}
avgtmax <- ny_noaa_clean %>% 
  filter(
    month %in% c("January", "July")
  ) %>% 
  drop_na(tmax) %>% 
  group_by(year, id, month) %>% 
  summarize(
    avg_tmax = mean(tmax, na.rm = TRUE)
  ) 

ggplot(avgtmax, (aes(x = year, y = avg_tmax, color = id))) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average maximum temperature vs. year by weather station in January and July",
    x = "Year",
    y = "Average maximum temperature (C)"
  ) +
  facet_grid(. ~ month) +
  theme(legend.position = "none")

```

The overall trends in these graphs are that in January, the average maximum temperature from 1980 to 2010 was between -10 and 10 degrees Celsius. We have a few outliers at about 11, -9, -13, and -12. In July, the average maximum temperature from 1980 to 2010 was between 20 and 35 degrees Celsius. Some of the outliers were 14, 18, 19, and 36.  


Now let's make a two-panel plot showing (i) tmax vs tmin for the full dataset and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r diff_two_plot}

temp_plot = 
ny_noaa_clean %>% 
ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
    labs(
    title = "Minimum and maximum temperatures",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)"
    )


snow <- ny_noaa_clean %>% 
  filter(snow > 0 & snow < 100) %>%
  mutate(snow = as.numeric(snow)) %>%
  mutate(year = as.factor(year))

snow_plot =   
ggplot(snow, aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    title = "Snowfall values by year",
    x = "Snowfall (mm)",
    y = "Year"
    )

temp_plot + snow_plot
  
```

In these plots, we see that there is large number of days in which the maximum temperature and minimum temperatures were between 15 for tmin and 30 for tmax and -15 for tmin and -5 for tmax. For the snowfall plot, from 1981 to 2010, most of the days with snowfall have a value between 0 and 30 mm. There is also another large set of days that had snowfall values between 40 and 60 mm and another between 70 and 80 mm.

## Iteration coding:

### Homicide data:

The Washington Post has gathered data on homicides in 50 large U.S. cities and that is the data used in this analysis.

```{r}
homicides = read.csv("./data/homicide-data.csv")

```

The raw dataset `homicides` contains `r ncol(homicides)` variables and `r nrow(homicides)` observations. It describes information on homicides from 50 large US cities. Variables of interest include the case ID, the date of the incident, information on the victim (name, race, age and sex), information on the location of the homicide, and the disposition of the case. 

We will create a new variable `city_state`.

```{r}
new_homi <- homicides %>%
  unite("city_state", city:state, sep = ", ", remove = FALSE)
```

We will first look at the total number of homicides in a city and then at the number of homicides that are unsolved in a city.

```{r}
new_homi %>% 
  group_by(city) %>%
  summarize(count = n()) %>%
  knitr::kable(digits = 1)

new_homi %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  group_by(city) %>%
  summarize(count = n()) %>%
  knitr::kable(digits = 1)
```


Let's focus on crime in just Baltimore, MD:


```{r}
new_homi %>%
  filter(city_state == "Baltimore, MD") %>%
  summarize(count = n())

new_homi %>%
  filter(city_state == "Baltimore, MD") %>%
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  summarize(count = n())

```

From the above code, we see that in the city of Baltimore, MD, there were a total of 2827 homicides and of those homicides, 1825 were unsolved.

Let's use the `prop.test` function on just Baltimore, MD to estimate the proportion of homicides that are unsolved and its confidence interval.

```{r}
Balt_test <- prop.test(1825, 2827) %>%
  broom::tidy() %>%
  select(estimate, starts_with("conf"))
```

The proportion of homicides that are unsolved is 0.646 with a confidence interval of 0.628 and 0.663.

Now we will create a function to do this for all the cities in the dataset:

```{r}
homi_nest =
  new_homi %>% 
  relocate(city_state) %>% 
  nest(data = uid:disposition)

prop = function(df) {

  data_result = df %>% 
    summarize(total = n(),
              unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))
    
  prop_results = 
    prop.test(x = data_result %>% pull(unsolved), 
              n = data_result %>% pull(total))

  return(prop_results)
}


final <- homi_nest %>% 
  mutate(results = map(data, prop),
         estimates = map(results, broom::tidy)) %>% 
  select(city_state, estimates) %>% 
  unnest(estimates) %>%
  select(city_state, estimate, starts_with("conf"))


final %>% 
  filter(city_state != "Tulsa, AL") %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
    geom_line() +
    geom_point()+
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=.2,
                 position=position_dodge(0.05))+
    theme(axis.text.x = element_text(angle = 90))

```

The above plot shows the proportion of unsolved homicides in each city with its corresponding confidence interval. There was an error in the data where Tulsa, AL was incorrected entered for Tulsa, OK. Instead of completely removing it from the dataset, I filtered it out of the plot.

## Regression coding:

### Homicide data:

We will start by loading the homicide data. 

```{r}
homicides_two = read.csv("./data/homicide-data.csv")
```

Next we will do some data cleaning as below:

```{r}
homicides_two =   
  homicides_two %>%
  unite("city_state", city:state, sep = ", ", remove = FALSE) %>% 
  mutate(solved = as.numeric(disposition == "Closed by arrest")) %>%
  filter(!(city %in% c("Dallas", "Phoenix", "Kansas City"))) %>%
  filter(city_state != "Tulsa, AL") %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  filter(victim_race %in% c("White", "Black")) %>%
  mutate(victim_race = fct_relevel(victim_race, "White")) %>%
  filter(victim_sex != "Unknown")

```

Now for just the city of Baltimore, we will fit a logistic model with homicide being solved as the outcome and with victim age, sex, and race as predictors. 

```{r}

Balt =
  homicides_two %>%
  filter(city == "Baltimore") %>%
  select(solved, victim_age, victim_race, victim_sex)

fit_balt = 
  Balt %>% 
  glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

fit_balt %>% 
  broom::tidy() %>%
  mutate(lower_CI = exp(estimate - 1.96*std.error),
         upper_CI = exp(estimate + 1.96*std.error),
         OR = exp(estimate)) %>%
  select(term, OR, lower_CI, upper_CI) %>% 
  knitr::kable(digits = 3)


```

Let's repeat this for all the cities and get the OR with their 95% CI for solving homicides comparing males to females, adjusting for race and age.  

```{r}
homi_nest =
  homicides_two %>% 
  select(city_state, solved, victim_age, victim_race, victim_sex) %>% 
  relocate(city_state) %>% 
  nest(data = solved:victim_sex)


fit_all = 
  homi_nest %>% 
  mutate(
    models = map(.x = data, ~glm(solved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  mutate(lower_CI = exp(estimate - 1.96*std.error),
         upper_CI = exp(estimate + 1.96*std.error),
         OR = exp(estimate)) %>%
  select(city_state, term, OR, lower_CI, upper_CI) %>%
  filter(term == "victim_sexMale")

fit_all %>%
  knitr::kable(digits = 3)

```

Now we will make a plot showing the OR and 95% CI for each city.

```{r}

fit_all %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +        
    geom_point() +
    geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI)) +
    theme(axis.text.x = element_text(angle = 80, hjust = 1))

```

From this plot, we can see the city with the lowest OR for solved homicides comparing male victims to females while adjusting for race and age is New York at 0.26 and the highest is Albuquerque at 1.77.

**The interpretations are as follows:**

**In New York City, the odds of solving a homicide case for a male is 0.26 times the odds of solving a homicide case for a female, adjusting for race and age. In Albuquerque, the odds of solving a homicide case for a male is 1.77 times the odds of solving a homicide case for a female, adjusting for race and age.**

**However, the confidence interval for Albuquerque includes the null value of 1 making the OR not statistically significant whereas 1 is not in the confidence interval for New York, which means the OR is statistically significant. We would need to double check with the p-values.**
