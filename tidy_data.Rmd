---
title: "R Programming Code"
output: github_document
editor_options: 
  chunk_output_type: console
---

## Preparation:

We'll start by having a code chunk in the beginning that loads all the packages we will need and set up formatting for visualizations:

```{r setup, include = FALSE}
library(tidyverse)
library(readxl)
library(p8105.datasets)
library(hexbin)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Data cleaning and Tidying:

### NYC Transit dataset cleaning:

This dataset contains information related to each entrance and exit for each subway station in NYC.

We will start by reading in the datafile using the `readr` function from the `tidyverse` package and cleaning the data by using the `clean_names` function from the `janitor` package. We will also retain certain variables and convert the entry variable from a character variable to a logical variable. 

```{r subway}
subway = read_csv("./data/Subway.csv") %>% 
    janitor::clean_names() %>% 
    select(line, station_name, station_latitude, station_longitude, 
      starts_with("route"), entry, exit_only, vending, entrance_type, ada) %>% 
    mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) %>% 
    mutate(route8 = as.character(route8)) %>%  
    mutate(route9 = as.character(route9)) %>% 
    mutate(route10 = as.character(route10)) %>% 
    mutate(route11 = as.character(route11)) 

subway

```

This dataset contains 20 columns and 1868 rows It has the 20 variables that we selected it to keep. We imported the file, used the `clean_names` function to do a quick clean. Then we selected what variables we wanted to keep. Some of the route variables were in `dbl` format instead of `chr` like most of the route variables so we changed that. And lastly we turned the entry variable from character into a logical variable. 

This data is not tidy because the route variables should be converted from a wide to long format.

We can use the following code to find the number of distinct stations:

```{r}
subway %>% 
  select(station_name, line) %>% 
  distinct
```

There are 465 distinct stations.

We can use the following code to find the number of ADA compliant stations:

```{r}
subway %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

There are 84 ADA compliant stations.

We can use the following code to find the proportion of station entrances/exits without vending allow entrance:

```{r}
subway %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

The proportion is 0.377.

We can use the following code to find how many stations serve the A train and of the stations that serve the A train, how many are ADA compliant:

```{r}
subway %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct

subway %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

There are 60 stations that serve the A train and of those, 17 are ADA compliant.

### Mr. Trash Wheel Dataset cleaning:

Let's start by reading and cleaning the Mr. Trash Wheel and Professor Trash Wheel datasets. 

**Make sure to import the correct sheet from the Excel file:**

```{r Trashwheel}
trash = read_excel("./data/Trash_Wheel.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N549") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(sports_balls = as.integer(round(sports_balls))) %>% 
  mutate(ID = "A")
```

```{r Professor}
professor = read_excel("./data/Trash_Wheel.xlsx", sheet = "Professor Trash Wheel", range = "A2:M96") %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(ID = "B")
```

Next we will combine both datasets into one dataset.

```{r combo}
combo = merge(x = trash, y = professor, all = TRUE) %>% 
  select(ID, everything())
```

The new and combined dataset is a full merge and has 641 observations (`nrow(combo)`) and 15 variables (`ncol(combo)`). All the variables exist in both sets except for the _sports_balls_ variable; it came from the **trash** dataset. We can distinguish which observation is from which dataset by the _ID_ variable; an _ID_ value equal to A is for the **trash** dataset and an _ID_ value of B is for the **professor** dataset. 

To find the total weight of trash collected by Professor Trash Wheel, we can use the following code: `sum(subset(combo, ID == "B")$weight_tons)`, which gives us the sum of the _weight_tons_ variable restricted to the observations from the **Professor** dataset, identified by _ID = B_. The answer is 190.12 tons.

To find the total number of sports balls collected by Mr. Trash Wheel in 2020, we can use the following code: `sum(subset(combo, ID == "A" & year == "2020")$sports_balls)`, which gives us the sum of the _sports_balls_ variable restricted to the observations from the **Trash** dataset, identified by _ID = A_, and only in the year 2020. The answer is 856 sports balls.


### Code to merge multiple datasets:

Clean up the pols-month file:

```{r pols}
pols = read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
    janitor::clean_names() %>% 
    separate(mon, sep = "-", into = c("year", "month", "day")) %>%
    mutate(month = month.name[as.numeric(month)]) %>% 
    mutate(year = as.numeric(year)) %>% 
    mutate(month = factor(month, levels = month.name)) %>% 
    mutate(president = case_when(prez_gop == 1 ~ "gop", TRUE ~ "dem")) %>% 
    select(-day, -prez_gop, -prez_dem) %>% 
    arrange(year, month)
```

Clean up the snp file:

```{r snp}
snp = read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
    janitor::clean_names() %>% 
    separate(date, sep = "/", into = c("month", "day", "year")) %>% 
    mutate(month = month.name[as.numeric(month)]) %>% 
    mutate(century = case_when(year < 16 ~ 2000, TRUE ~ 1900)) %>% 
    mutate(year = as.numeric(year)) %>% 
    mutate(year = year + century) %>% 
    select(year, month, close) %>% 
    mutate(month = factor(month, levels = month.name)) %>% 
    arrange(year, month) 
```

Now tidy the unemployment file:

```{r unemploy}
unemploy = read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      jan:dec,
      names_to = "month",
      values_to = "percent_unemploy") %>% 
    mutate(month = as.factor(month)) %>% 
    mutate(month = month.name[as.numeric(month)]) %>% 
    mutate(month = as.factor(month)) %>% 
    mutate(month = factor(month, levels = month.name)) %>% 
    arrange(year, month)
```


Now we will create a merged dataset in 2 steps.

First we will merge `snp` into `pols`:

```{r first_merge}
first = left_join(pols, snp, by = c("year", "month"))
```

Then we will merge the `unemploy` file into this new merged `first` file:

```{r total}
total = left_join(first, unemploy, by = c("year", "month"))
```

The **snp** dataset has just 3 variables: the _close_ variable, which was untouched, and then _month_ and _year_ that we created by separating the date. The **pols** dataset has the same _month_ and _year_ that we made like in the **snp** dataset. It also has many of the original variables, as well as a new variable called _president_ which was created logically based off the _prez_dem_ and _prez_gop_ variables. The **unemploy** dataset has also 3 variables, with the _month_ and _year_ variables made by separating the date and the _percent_unemploy_ variable made by the `pivot_longer` function. 

The final dataset **total** has 822 observations (`nrow(total`) and 11 variables (`ncol(total`). We can find the range of years with the following code: `range(total$year)`, which is from 1947 to 2015. The key variables are _year_ and _month_ which was present in all 3 datafiles and was used to perform all the merges. 

## Exploratory Data Analysis (EDA):

### NOAA National Climatic Data Center for all weather stations in New York State:

Let's load the data:

```{r load_nynoaa}
data("ny_noaa")

ny_noaa
```

The dataset has 7 variables (`ncol(ny_noaa)`) and 2,595,176 observations (`nrow(ny_noaa)`). It has a combination of integer and character variables, with also a date variable. The variables that make up the dataset are an ID variable of the weather station, date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures in Celsius. There is a large number of missing data because each weather station may collect only a subset of these variables, so the dataset has observations with missing data. 

Let's now do some data cleaning of this dataset:

```{r clean_nynoaa}
ny_noaa_clean <- ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) %>%
  mutate_at(c(2:9), as.numeric) %>% 
  mutate(prcp = prcp/10) %>% 
  mutate(tmin = tmin/10, tmax = tmax/10) %>% 
  mutate(month = month.name[as.numeric(month)])
ny_noaa_clean
```

We cleaned up the data by cleaning the names, separating the variable for date of observation into the year, month, and day, converting all the variables except the ID into a numeric variable, and converting the `prcp`, `tmix`, and `tmax` variables from it's tenths value to it's whole value by dividing by 10.  

Now let's make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r two_plot}
avgtmax <- ny_noaa_clean %>% 
  filter(
    month %in% c("January", "July")
  ) %>% 
  drop_na(tmax) %>% 
  group_by(year, id, month) %>% 
  summarize(
    avg_tmax = mean(tmax, na.rm = TRUE)
  ) 

ggplot(avgtmax, (aes(x = year, y = avg_tmax, color = id))) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average maximum temperature vs. year by weather station in January and July",
    x = "Year",
    y = "Average maximum temperature (C)"
  ) +
  facet_grid(. ~ month) +
  theme(legend.position = "none")

```

The overall trends in these graphs are that in January, the average maximum temperature from 1980 to 2010 was between -10 and 10 degrees Celsius. We have a few outliers at about 11, -9, -13, and -12. In July, the average maximum temperature from 1980 to 2010 was between 20 and 35 degrees Celsius. Some of the outliers were 14, 18, 19, and 36.  


Now let's make a two-panel plot showing (i) tmax vs tmin for the full dataset and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r diff_two_plot}

temp_plot = 
ny_noaa_clean %>% 
ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
    labs(
    title = "Minimum and maximum temperatures",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)"
    )


snow <- ny_noaa_clean %>% 
  filter(snow > 0 & snow < 100) %>%
  mutate(snow = as.numeric(snow)) %>%
  mutate(year = as.factor(year))

snow_plot =   
ggplot(snow, aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    title = "Snowfall values by year",
    x = "Snowfall (mm)",
    y = "Year"
    )

temp_plot + snow_plot
  
```

In these plots, we see that there is large number of days in which the maximum temperature and minimum temperatures were between 15 for tmin and 30 for tmax and -15 for tmin and -5 for tmax. For the snowfall plot, from 1981 to 2010, most of the days with snowfall have a value between 0 and 30 mm. There is also another large set of days that had snowfall values between 40 and 60 mm and another between 70 and 80 mm.



